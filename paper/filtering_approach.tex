\section{Filtering Approach} \label{filtering_approach}

This section does not only explain the concept of our proposed filtering approach, but also describes how to integrate our filter into the well-known and widely-used sliding window technique, as shown in Figure \ref{fig:swf}.

In general, the sliding window filter considers the most recent measurements in a data stream. The considered measurements are usually passed to a classifier, which aims at categorizing the current time series subsequence. In case that the current subsequence was assigned to a known category or class, a corresponding action is triggered and the next non-overlapping window, $w$ steps along the time axis, is examined. If the current subsequence just contains noise and no category was assigned, the next overlapping window, $s$ steps along the arrow of time, is processed. The main limitation of this traditional sliding window technique is its computational complexity, which increases with growing window size, shrinking step size, higher sample rate, and larger training set. %This is due to the fact that more windows require more classifications, which in turn often involve computational expensive operations.

For instance, given a data stream of length $l {=} 10090$, a window size of $w {=} 100$, and a step size of $s {=} 10$, we need to classify $(l - (w - s)) / s = 1000$ windows. Moreover, assuming $20$ training time series, classifying $1000$ windows by means of the nearest neighbor approach requires exactly $20*1000=20K$ dissimilarity comparisons. In case that we employ unconstrained DTW as time series distance measure, we need to compute $20K$ full warping matrices, each of them containing $w*w = 10K$ cells, resulting in a total amount of $200M$ distance operations.


\tikzstyle{decision} = [diamond, draw, fill=white!20, text width=5.5em, text badly centered, node distance=2cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=white!20, text width=5em, text centered, minimum height=4em]
\tikzstyle{line} = [draw, -latex']

\begin{figure}
    \begin{center}
        \resizebox {\textwidth} {!} {
            \begin{tikzpicture}[node distance = 1.5cm, auto]
                \node [block] (sod) {sensors or devices};
                \node [block, right of=sod, node distance=5cm, text width=2cm] (extract) {Extract last subsequence from Q of size $w$, $Q[t-w,t]$};
                \node [block, draw=blue, right of=extract, node distance=4cm, text width=2cm] (filter) {Time series filter};
                \node [decision, draw=blue, right of=filter, node distance=3cm] (filterdecide) {$Q[t-w,t]$ can pass?};
                \node [block, right of=filterdecide, node distance=3.5cm, text width=2cm] (nnc) {Time series classificator};
                \node [decision, below of=nnc, node distance=2.4cm] (decide) {$Q[t-w,t]$ classifiable?};
                \node [block, below of=filterdecide, node distance=2.4cm] (sleeps) {Sleep for $s$ time};
                \node [block, below of=decide, node distance=2.4cm, text width=4cm] (action) {Trigger event that $Q[t-w,t]$ has been classified and sleep for $w$ time};

                \path [line,dashed] (sod) -- node[text width=2.5cm] (ctss) {Continuous time series stream $Q$} (extract);
                \path [line] (extract) -- node {$Q[t-w,t]$} (filter);
                \path [line] (filter) -- (filterdecide);
                \path [line] (filterdecide) -- node {yes} (nnc);
                \path [line] (filterdecide) -- node [near start] {no} (sleeps);
                \path [line] (nnc) -- (decide);
                \path [line] (decide) -- node {no} (sleeps);
                \path [line,dashed] (sleeps) -| (ctss);
                \path [line] (decide) -- node {yes} (action);
                \path [line,dashed] (action) -| (ctss);
            \end{tikzpicture}
        }
    \end{center}
    \caption{Flowchart of sliding window technique with filter, highlighted in blue. The current time is denoted by $t$. Window and step size are denoted by $w$ and $s$ respectively.}
    \label{fig:swf}
\end{figure}

In order to reduce the large number of computational expensive dissimilarity comparisons, we propose to employ a sliding window filter, which is capable of separating signal from noise, only passing promising time series subsequences to the classifier. In that sense, the proposed filter can also be considered as a binary classifier, which prunes windows that are likely to be noise and forwards subsequences that exhibit similar features as the training time series. Extracting characteristic time series features that can be used as a filter criterion should ideally exhibit linear complexity, because we aim at replacing more expensive dissimilarity comparisons. Suitable filter candidates include statistical measures, such as the sample variance \cite{chan1983algorithms} and length normalized complexity estimate \cite{batista2011complexity}, explained in more detail below.

Given is a time series $Q = (q_1, \dots, q_n)$ with length $n$, we can define the sample variance ($VAR$) and length normalized complexity estimate ($LNCE$) as follows:

\begin{equation*}
    VAR(Q) = \frac{1}{n}\sum \limits_{i=1}^{n} (q_i - \mu)^2
\end{equation*}

\begin{equation*}
    LNCE(Q) = \frac{1}{n-1}\sqrt[2]{\sum \limits_{i=1}^{n-1} (q_i - q_{i + 1})^2}
\end{equation*}

Having defined the above statistical measures, we are able to compute the $VAR$ and $LNCE$ for all training time series and, subsequently, use the resulting range of statistical values to learn an appropriate filter interval. During testing, each window that exhibits a measured value within the learned interval is passed to the classifier or pruned otherwise. In order to avoid excessive pruning of relevant windows, we further more introduce a multiplication factor, which allows us to expend the interval boundaries by a certain percentage.

In general, we aim at designing a filter with high precision and recall. In our case, precision is the ratio between the number of relevant windows that were passed to the classifier (true positives) and the number of all windows that were passed to the classifier (true positives and false positives). Consequently, recall is the ratio between the number of relevant windows that were passed to the classifier (true positives) and the number of all relevant windows (true positives and false negatives). An exhaustive evaluation of our proposed sliding window filter in dependence of all model parameters is presented in the next section.
