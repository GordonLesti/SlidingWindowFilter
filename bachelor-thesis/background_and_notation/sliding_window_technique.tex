\subsection{Sliding Window Technique} \label{sliding_window_technique}
Given is a continuous time series data stream $Q$. The last subsequence of size $w$ is in the interest of the sliding
window technique. All subsequences or time series data points of $Q$ that are back even further as the window size $w$
are in this moment irrelevant. The sliding window application tries to classify this last subsequence of $Q$ with the
help of a time series classificator. This time series classificator makes its decisions based on a given time series
training set. The application triggers an event in the case of a positive classification of the subsequence and an other
application can react on this event.

The sliding window application remains idle until the time series data stream $Q$
grows. Afterwards, the process is repeated again. However, the sliding window application waits until the time series
data stream $Q$ has grown by a predefined step size $s$. If necessary, this stepwise approach can avoid the execution of
the process continuously after every new data point in $Q$. Figure \ref{fig:swt} illustrates the described process.

\tikzstyle{decision} = [diamond, draw, aspect=2, fill=white!20, text width=8em, text badly centered, node distance=2cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=white!20, text width=5em, text centered, minimum height=4em]
\tikzstyle{line} = [draw, -latex']

\begin{figure}
    \begin{center}
        \resizebox {\textwidth} {!} {
            {\tiny
                \begin{tikzpicture}[node distance = 1.5cm, auto]
                    \node [block] (sod) {sensors or devices};
                    \node [block, right of=sod, node distance=6cm, text width=2cm] (extract) {Extract last subsequence from Q of size $w$, $Q[t-w,t]$};
                    \node [block, right of=extract, node distance=4cm, text width=2cm] (nnc) {Time series classificator};
                    \node [decision, below of=nnc] (decide) {$Q[t-w,t]$ classifiable?};
                    \node [block, left of=decide, node distance=3cm] (sleeps) {Sleep for $s$ time};
                    \node [block, below of=decide, node distance=2cm, text width=2cm] (action) {Trigger event that $Q[t-w,t]$ has been classified and sleep for $w$ time};

                    \path [line,dashed] (sod) -- node (ctss) {Continuous time series stream $Q$} (extract);
                    \path [line] (extract) -- node {$Q[t-w,t]$} (nnc);
                    \path [line] (nnc) -- (decide);
                    \path [line] (decide) -- node {no} (sleeps);
                    \path [line,dashed] (sleeps) -| (ctss);
                    \path [line] (decide) -- node {yes} (action);
                    \path [line,dashed] (action) -| (ctss);
                \end{tikzpicture}
            }
        }
    \end{center}
    \caption{Possible design for a sliding window application. The current time is stored in variable $t$. The constant
    variables $w$ for the window size and $s$ for the step size are predefined.}
    \label{fig:swt}
\end{figure}

The explained sliding window technique uses a generic time series classificator. However, in this bachelor thesis the
generic time series classificator is replaced by 1NN-DTW. A time series window $Q[t-w,t]$ is classified as instance of
class $K_i$ by 1NN-DTW if the DTW distance between $Q[t-w,t]$ and the nearest neighbour passes a predefined
class threshold $\epsilon_i$. The following subsection gives some background of DTW.
